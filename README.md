# Visual Question Answering (VQA) trÃªn Táº­p dá»¯ liá»‡u Äá»™ng váº­t

[cite_start]Äá»“ Ã¡n cuá»‘i ká»³ mÃ´n **Há»c SÃ¢u (Deep Learning)** - TrÆ°á»ng Äáº¡i há»c TÃ´n Äá»©c Tháº¯ng[cite: 1, 3].

Dá»± Ã¡n nÃ y xÃ¢y dá»±ng vÃ  so sÃ¡nh cÃ¡c mÃ´ hÃ¬nh Visual Question Answering (VQA) Ä‘á»ƒ tráº£ lá»i cÃ¡c cÃ¢u há»i tiáº¿ng Anh liÃªn quan Ä‘áº¿n hÃ¬nh áº£nh Ä‘á»™ng váº­t (vÃ­ dá»¥: "What animal is this?", "How many..."). [cite_start]Dá»± Ã¡n so sÃ¡nh hiá»‡u suáº¥t giá»¯a viá»‡c tá»± xÃ¢y dá»±ng mÃ´ hÃ¬nh CNN (Train from scratch) vÃ  sá»­ dá»¥ng mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n trÆ°á»›c (Pre-trained ResNet-50), Ä‘á»“ng thá»i Ä‘Ã¡nh giÃ¡ tÃ¡c Ä‘á»™ng cá»§a cÆ¡ cháº¿ Attention[cite: 911, 1022].

## ğŸ‘¥ ThÃ nh viÃªn thá»±c hiá»‡n
| STT | Há» vÃ  tÃªn | MSSV |
|:---:|:---|:---|
| 1 | Nguyá»…n Quá»‘c Duy | 52200196 |
| 2 | Nguyá»…n HoÃ ng Ã‚n | 52200183 |
| 3 | Nguyá»…n Nháº­t TrÆ°á»ng | 52200192 |

**Giáº£ng viÃªn hÆ°á»›ng dáº«n:** PGS.TS. [cite_start]LÃª Anh CÆ°á»ng[cite: 6].

## ğŸ“‚ Cáº¥u trÃºc Dá»± Ã¡n
* [cite_start]`dataset.ipynb`: Notebook xá»­ lÃ½ dá»¯ liá»‡u (táº£i áº£nh tá»« COCO, lá»c cÃ¢u há»i tá»« VQA v2.0, tiá»n xá»­ lÃ½ vÄƒn báº£n)[cite: 826].
* [cite_start]`model_training.ipynb`: Huáº¥n luyá»‡n mÃ´ hÃ¬nh **Train from Scratch** (Tá»± xÃ¢y dá»±ng CNN)[cite: 965].
* [cite_start]`model_pre-trained.ipynb`: Huáº¥n luyá»‡n mÃ´ hÃ¬nh **Pre-trained** (Sá»­ dá»¥ng ResNet-50)[cite: 958].
* `midterm_report.pdf`: BÃ¡o cÃ¡o chi tiáº¿t vá» phÆ°Æ¡ng phÃ¡p vÃ  káº¿t quáº£ thá»±c nghiá»‡m.

## ğŸ“Š Dá»¯ liá»‡u (Dataset)
Dá»± Ã¡n sá»­ dá»¥ng káº¿t há»£p hai bá»™ dá»¯ liá»‡u lá»›n, Ä‘Æ°á»£c lá»c riÃªng cho category **Animal**:
1.  **HÃ¬nh áº£nh:** Táº­p **COCO train 2014**. [cite_start]ÄÆ°á»£c lá»c Ä‘á»ƒ chá»‰ láº¥y cÃ¡c áº£nh chá»©a Ä‘á»™ng váº­t Ä‘á»ƒ trÃ¡nh dá»¯ liá»‡u thÆ°a[cite: 704].
2.  **CÃ¢u há»i/Tráº£ lá»i:** Táº­p **VQA v2.0 (2017)**. [cite_start]Lá»c cÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n nháº­n dáº¡ng ("what animal is") vÃ  Ä‘áº¿m sá»‘ lÆ°á»£ng ("how many")[cite: 712, 760].

**Quy trÃ¬nh xá»­ lÃ½:**
* [cite_start]**áº¢nh:** Resize vá» kÃ­ch thÆ°á»›c `224x224`, chuyá»ƒn thÃ nh Tensor vÃ  chuáº©n hÃ³a (Normalize) theo ImageNet[cite: 879, 881].
* [cite_start]**VÄƒn báº£n:** Tokenization, táº¡o tá»« Ä‘iá»ƒn (Vocab), chuyá»ƒn thÃ nh vector vÃ  padding Ä‘á»™ dÃ i cá»‘ Ä‘á»‹nh[cite: 893, 907].

## ğŸ—ï¸ Kiáº¿n trÃºc MÃ´ hÃ¬nh
Dá»± Ã¡n thá»±c nghiá»‡m 4 cáº¥u hÃ¬nh mÃ´ hÃ¬nh khÃ¡c nhau dá»±a trÃªn sá»± káº¿t há»£p cá»§a cÃ¡c thÃ nh pháº§n sau:

### 1. TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng áº£nh (Image Feature Extraction)
* [cite_start]**Custom CNN (Train from Scratch):** Máº¡ng tÃ­ch cháº­p tá»± xÃ¢y dá»±ng gá»“m 3 lá»›p Conv2d, BatchNorm vÃ  MaxPool[cite: 965, 969].
* [cite_start]**ResNet-50 (Pre-trained):** Sá»­ dá»¥ng máº¡ng ResNet-50 Ä‘Ã£ huáº¥n luyá»‡n trÃªn ImageNet, Ä‘Ã³ng bÄƒng trá»ng sá»‘ vÃ  láº¥y Ä‘áº·c trÆ°ng táº¡i lá»›p trÆ°á»›c Fully Connected (kÃ­ch thÆ°á»›c 2048)[cite: 960].

### 2. Xá»­ lÃ½ ngÃ´n ngá»¯ (Question Processing)
* [cite_start]Sá»­ dá»¥ng máº¡ng **LSTM** (Long Short-Term Memory) Ä‘á»ƒ xá»­ lÃ½ chuá»—i tá»«[cite: 987].
* [cite_start]Sá»­ dá»¥ng **Word Embeddings** (GloVe)[cite: 657].

### 3. CÆ¡ cháº¿ Attention
* [cite_start]Sá»­ dá»¥ng cÆ¡ cháº¿ Attention Ä‘Æ¡n giáº£n Ä‘á»ƒ trá»ng sá»‘ hÃ³a thÃ´ng tin tá»« LSTM, giÃºp mÃ´ hÃ¬nh táº­p trung vÃ o cÃ¡c pháº§n quan trá»ng cá»§a cÃ¢u há»i vÃ  hÃ¬nh áº£nh[cite: 991, 1124].

## ğŸ“ˆ Káº¿t quáº£ Thá»±c nghiá»‡m

DÆ°á»›i Ä‘Ã¢y lÃ  tÃ³m táº¯t káº¿t quáº£ huáº¥n luyá»‡n sau 50 epochs:

| MÃ´ hÃ¬nh | CÆ¡ cháº¿ | Train Accuracy | Val Accuracy | Nháº­n xÃ©t |
|:---|:---:|:---:|:---:|:---|
| **Train from Scratch** | Non-Attention | ~78% | ~33% | [cite_start]Overfitting náº·ng, Validation Loss tÄƒng dáº§n sau epoch 25[cite: 1153, 1154]. |
| **Train from Scratch** | **Attention** | ~80% | ~32% | [cite_start]Attention giÃºp há»c nhanh hÆ¡n nhÆ°ng váº«n bá»‹ overfitting náº·ng[cite: 1193, 1194]. |
| **Pre-trained (ResNet)**| Non-Attention | ~64% | ~37% | [cite_start]á»”n Ä‘á»‹nh hÆ¡n, tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n mÃ´ hÃ¬nh tá»± xÃ¢y[cite: 1040, 1041]. |
| **Pre-trained (ResNet)**| **Attention** | **~73%** | **~41%** | **Káº¿t quáº£ tá»‘t nháº¥t**. [cite_start]Attention giÃºp cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c trÃªn táº­p Validation[cite: 1250, 1288]. |

## ğŸš€ HÆ°á»›ng dáº«n CÃ i Ä‘áº·t & Cháº¡y
### YÃªu cáº§u há»‡ thá»‘ng
* Python 3.10+
* ThÆ° viá»‡n: `torch`, `torchvision`, `pandas`, `numpy`, `matplotlib`, `nltk`, `tqdm`, `Pillow`.

### CÃ¡c bÆ°á»›c thá»±c hiá»‡n
1.  **Clone repository:**
    ```bash
    git clone https://github.com/quocduy1234231/Animal-VQA-DeepLearning.git
    ```

2.  **Táº£i dá»¯ liá»‡u:**
    Cháº¡y file `dataset.ipynb` Ä‘á»ƒ táº£i áº£nh tá»« COCO API vÃ  file json tá»« VQA, sau Ä‘Ã³ tiá»n xá»­ lÃ½ táº¡o ra file `dataset_vqa.csv`.

3.  **Huáº¥n luyá»‡n:**
    * Äá»ƒ train mÃ´ hÃ¬nh tá»± xÃ¢y dá»±ng: Cháº¡y `model_training.ipynb`.
    * Äá»ƒ train mÃ´ hÃ¬nh ResNet-50: Cháº¡y `model_pre-trained.ipynb`.
    * *LÆ°u Ã½: CÃ³ thá»ƒ báº­t/táº¯t biáº¿n `use_attention = True/False` trong code Ä‘á»ƒ thá»­ nghiá»‡m cÃ¡c cáº¥u hÃ¬nh khÃ¡c nhau.*

## ğŸ”— Tham kháº£o
* [cite_start][VQA Dataset Website](https://visualqa.org/) [cite: 1417]
* [cite_start][MS COCO Dataset](https://cocodataset.org/) [cite: 1419]
* [Pytorch Documentation](https://pytorch.org/docs/stable/index.html)